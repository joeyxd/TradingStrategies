import pandas as pd
import os

# --- 1. Configuration ---
# Set the directory where you uploaded all your daily CSV files.
data_directory = 'data/'

# Set the name for your final, combined, and cleaned output file.
output_path = 'data/MNQ_Combined_Clean.parquet'

# --- 2. Data Loading and Consolidation ---
# Create an empty list to hold the data from each daily file.
all_dataframes = []

print(f"Scanning for CSV files in '{data_directory}'...")

# Loop through every file in the specified directory.
for filename in os.listdir(data_directory):
    if filename.endswith('.csv'):
        # Construct the full path to the file.
        file_path = os.path.join(data_directory, filename)
        print(f"  -> Reading {filename}...")
        
        # Read the daily CSV file into a DataFrame.
        daily_df = pd.read_csv(file_path)
        
        # Add the DataFrame to our list.
        all_dataframes.append(daily_df)

print(f"\nFound and read {len(all_dataframes)} daily CSV files.")

# Combine all the DataFrames in the list into one single DataFrame.
df = pd.concat(all_dataframes, ignore_index=True)

print("All files have been combined into a single dataset.")


# --- 3. Cleaning and Structuring ---
print("\nStarting data cleaning process...")

# Some grid exports from NinjaTrader have a redundant "Time" column. We'll use "Date".
# The "Date" column already contains the full timestamp.
df['datetime'] = pd.to_datetime(df['Date'])

# Set the new 'datetime' column as the DataFrame's index.
df.set_index('datetime', inplace=True)

# Sort the entire dataset by the timestamp. This is crucial after combining files.
df.sort_index(inplace=True)

# Standardize column names to lowercase.
df.columns = [col.lower() for col in df.columns]

# Select only the essential columns for our analysis.
# We also check for a 'volume' column. If not present, we create a dummy one.
required_cols = ['open', 'high', 'low', 'close']
if 'volume' in df.columns:
    required_cols.append('volume')

final_df = df[required_cols]

# If volume wasn't in the original file, add a column of zeros as a placeholder.
if 'volume' not in final_df.columns:
    final_df['volume'] = 0
    print("No 'Volume' column found. A placeholder column has been added.")

print("Data cleaning complete.")

# --- 4. Final Verification and Saving ---
print("\nFinal DataFrame Information:")
final_df.info()

print("\nTop 5 rows of final data (chronological):")
print(final_df.head())

print("\nBottom 5 rows of final data (chronological):")
print(final_df.tail())

# Save the final, cleaned DataFrame to a high-performance Parquet file.
final_df.to_parquet(output_path)

print(f"\nâœ… Success! Your consolidated and cleaned data has been saved to:\n{output_path}")